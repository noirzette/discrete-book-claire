<?xml version="1.0" encoding="UTF-8"?>
<section xml:id="s-some-common-rrs">
<title>Some Common Recurrence Relations</title>

<introduction><p>In this section we intend to examine a variety of recurrence relations that are not finite-order linear with constant coefficients. For each part of this section, we will consider a concrete example, present a solution, and, if possible, examine a more general form of the original relation.</p></introduction>

<subsection xml:id="rr-basic-example"><title>A  First Basic Example</title>
<p> Consider the homogeneous first-order linear relation without constant coefficients,  \(S(n) - n S(n - 1) = 0\), \(n \geq  1\), with initial condition \(S(0)
= 1\). Upon close examination of this relation, we see that the <m>n</m>th term is <m>n</m> times the \((n - 1)^{st}\) term, which is a property of <m>n</m> factorial. \(S(n) = n!\) is a solution of this relation, for if \(n \geq  1\),

\[S(n) = n! = n \cdot (n -1)! = n\cdot  S(n - 1)\]

In addition, since \(0! = 1\), the initial condition is satisfied. It should be pointed out that from a computational point of view, our <q>solution</q>
really isn't much of an improvement since the exact calculation of \(n!\) takes \(n-1\) multiplications.</p>

<p>If we examine a similar relation, \(G(k) - 2 ^k G(k - 1),\) \(k\geq 1\) with \(G(0) = 1\), a table of values for <m>G</m> suggests a possible solution:

 \[\begin{array}{ccccccc}
 k &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\
\hline
 G(k) &amp; 1 &amp; 2 &amp; 2^3 &amp; 2^6 &amp; 2^{10} &amp; 2^{15} \\
\end{array}\]

The exponent of 2 in \(G(k)\) is growing according to the relation \(E(k) = E(k - 1) + k,\) with \(E(0) = 0\). Thus \(E(k)=\frac{k(k+1)}{2}\)
and \(G(k) = 2^{k(k+1)/2}\). Note that \(G(k)\) could also be written as \(2^0 2^1 2^2 \cdots 2^k\), for \(k \geq  0\), but this is not a closed form expression.</p>

<p>In general, the relation \(P(n) = f(n)P(n - 1)\) for \(n \geq 1\) with \(P(0) =f(0)\), where <m>f</m> is a function that is defined for all \(n\geq
0\), has the <q>solution</q>


\[P(n)=\prod _{k=0}^n f(k)\]


This product form of \(P(n)\) is not a closed form expression because as <m>n</m> grows, the number of multiplications grow. Thus, it is really not a true solution.  Often, as for \(G(k)\) above, a closed form expression can be derived from the product form.</p>
</subsection>

<subsection xml:id="analysis-of-binary-search"><title>An Analysis of the Binary Search Algorithm</title>
<subsubsection><title></title>
<p> Suppose you intend to use a binary search algorithm (see <xref ref="ss-recursive-searching"/>) on lists of zero or more sorted items, and that the items are stored in an array, so that you have easy access to each item. A natural question to ask is <q>How much time will it take to complete the search?</q> When a question like this is asked, the time we refer to is often the so-called worst-case time. That is, if we were to search through <m>n</m> items, what is the longest amount of time that we will need to complete the search? In order to make an analysis such as this independent of the computer to be used, time is measured by counting the number of steps that are executed. Each
step (or sequence of steps) is assigned an absolute time, or weight; therefore, our answer will not be in seconds, but in absolute time units. If
the steps in two different algorithms are assigned weights that are consistent, then analyses of the algorithms can be used to compare their relative
efficiencies. There are two major steps that must be executed in a call of the binary search algorithm:</p>
<p><ol label="(1)">
<li><p> If the lower index is less than or equal to the upper index, then the middle of the list is located and its key is compared to the value that
you are searching for.</p></li>
<li><p>In the worst case, the algorithm must be executed with a list that is roughly half as large as in the previous execution. If we assume that
Step 1 takes one time unit and \(T(n)\) is the worst-case time for a list of <m>n</m> items, then

<mdn><mrow xml:id="eq-bin-search-recursion">T(n)= 1 + T (\lfloor n/2 \rfloor ),  \quad n>0</mrow></mdn>

For simplicity, we will assume that

<mdn><mrow xml:id="eq-bin-search-basis">T(0) = 0</mrow></mdn>


even though the conditions of Step 1 must be evaluated as false if \(n = 0\). You might wonder why \(n/2\) is truncated in <xref ref="eq-bin-search-recursion"/>. If <m>n</m> is
odd, then \(n = 2 k + 1\) for some \(k\geq  0\), the middle of the list will be the \((k + 1)^{st}\)  item, and no matter what half of the list the search is directed to, the reduced list will have \(k = \lfloor n/2\rfloor\) items. On the other hand, if <m>n</m> is even, then \(n
= 2 k\) for \(k>0\). The middle of the list will be the \(k^{th}\) item, and the worst case will occur if we are directed to the <m>k</m> items
that come after the middle (the \((k + 1)^{st}\) through \((2k)^{th}\) items). Again the reduced list has \(\lfloor n/2 \rfloor\) items.</p></li>
</ol></p>

<p><em>Solution to <xref ref="eq-bin-search-recursion"/>  and <xref ref="eq-bin-search-basis"/>.</em>  To determine \(T(n)\), the easiest case is when <m>n</m> is a power of two. If we compute \(T \left(2^m\right)\),
\(m\geq 0\) , by iteration, our results are

<me>\begin{array}{c}
T(1) = 1 + T(0) = 1\\
T(2) = 1 + T(1) = 2\\
T(4) = 1 + T(2) = 3\\
T(8) = 1 + T(4) = 4\\
\end{array}</me>

The pattern that is established makes it clear that \(T\left(2^m\right) = m + 1\). This result would seem to indicate that every time you double the size of your list, the search time increases by only one unit.</p>

<p>A more complete solution can be obtained if we represent <m>n</m> in binary form. For each \(n\geq 1\), there exists a non-negative integer <m>r</m> such that

<mdn><mrow xml:id="eq-inequality-84">\quad \quad 2^{r-1}\leq n &lt; 2^r</mrow></mdn> 

For example, if \(n = 21\), \(2^4 \leq  21 &lt; 2^5\); therefore, \(r = 5\). If <m>n</m> satisfies (8.4c), its binary representation requires \(r\) digits. For example, \(21_{\text{ten}}\) = \(10101_{\textrm{two}}\).</p>

<p>In general, \(n = \left(a_1a_2\ldots  a_r\right)_{\textrm{two}}\). where \(a_1=1\). Note that in this form, \(\lfloor n/2\rfloor\) is easy to describe:
it is the \(r-1\) digit binary number \(\left(a_1a_2\ldots  a_{r-1}\right)_{\textrm{two}}\)</p>

<p>Therefore,

<me>
\begin{split}
T(n) &amp;= T\left(a_1a_2\ldots  a_r\right)\\
	&amp; =1+ T\left(a_1a_2\ldots  a_{r-1}\right)\quad \\
	&amp; =1+\left(1+ T\left(a_1a_2\ldots  a_{r-2}\right)\right)\\
	&amp; =2+ T\left(a_1a_2\ldots  a_{r-2}\right)\\
	&amp;\quad  \vdots \\
	&amp; = (r-1) + T\left(a_1\right)\\
	&amp; = (r-1)+1\quad \textrm{ since } T(1)=1\\
	&amp; = r
\end{split}
</me>.</p>

<p>From the pattern that we've just established, \(T(n)\) reduces to <m>r</m>. A formal inductive proof of this statement is possible. However, we
expect that most readers would be satisfied with the argument above. Any skeptics are invited to provide the inductive proof. </p>
<p>For those who prefer to see a numeric example, suppose \(n = 21\).

<me>
\begin{split}
T(21) &amp;= T(10101) \\
		&amp; = 1 + T(1010) \\
		&amp;  = 1 + (1 + T(101)) \\
		&amp;  = 1 + (1 + (1 + T(10))) \\
		&amp;  = 1 + (1 + (1 + (1 + T(1))))\\
		&amp;  = 1 + (1 + (1 + (1 + (1 +T(0)))))\\
		&amp;  = 5
\end{split}
</me>
</p>

<p>Our general conclusion is that the solution to <xref ref="eq-bin-search-recursion"/>  and <xref ref="eq-bin-search-basis"/> is that for \(n\geq 1\), \(T(n) = r\), where \(2^{r-1}\leq n &lt; 2^r\).</p>

<p>A less cumbersome statement of this fact is that \(T(n)=\left\lfloor \log_{2}n\right\rfloor +1\). For example, \(T(21) = \left\lfloor \log_2 21\right\rfloor + 1 = 4 + 1 = 5\). </p>
</subsubsection>
<subsubsection xml:id="sss-review-of-logarithms"><title>Review of Logarithms</title><idx>Logarithms</idx>
<p>Any discussion of logarithms must start by establishing a base, which can be any positive number other than 1. With the exception of <xref ref="theorem-logs-related" text="type-local" />,
our base will be 2. We will see that the use of a different base (10 and \(e\approx  2.171828\) are the other common ones) only has the effect of
multiplying each logarithm by a constant. Therefore, the base that you use really isn't very important. Our choice of base 2 logarithms is convenient
for the problems that we are considering.</p>



<definition xml:id="def-log-base-2"><title>Base 2 logarithm</title><idx>Logarithm, base 2</idx><statement><p>The base 2 logarithm of a positive number represents an exponent and is defined by the following equivalence for any positive real numbers <m>a</m>.
\[\log_2 a = x \quad \Leftrightarrow \quad 2^x= a\]. </p></statement></definition>

<figure xml:id="fig-log-2-plot">
		 <caption>Plot of the logarithm, bases 2, function</caption>
		 <image width="100%" source="images/fig-log-2-plot.png">
		 <description>Plot of the logarithm, bases 2, function</description>
		 </image>
</figure>


<p>For example, \(\log_2 8 = 3\) because \(2^3 = 8\) and \(\log_21.414\approx 0.5\) because \(2^{0.5}\approx 1.414\). A graph of the function \(f(x)
= \log_2 x\) in <xref ref="fig-log-2-plot" text="type-global"/> shows that if \(a &lt; b\), the \(\log_2a &lt; \log_2 b\); that is, when <m>x</m> increases, \(\log_2 x\) also increases.
However, if we move <m>x</m> from \(2^{10} = 1024\) to \(2^{11} = 2048\), \(\log_2 x\) only increases from 10 to 11. This slow rate of increase
of the logarithm function is an important point to remember. An algorithm acting on <m>n</m> pieces of data that can be executed in \(\log_2{n}\)
time units can handle significantly larger sets of data than an algorithm that can be executed in \(n/100\) or  \(\sqrt{n}\) time units. The graph of \(T(n)=\left\lfloor \log_2n\right\rfloor +1\) would show the same behavior.</p>

<p>A few more properties that we will use in subsequent discussions involving logarithms are summarized in the following theorem.</p>

<theorem xml:id="theorem-log-properties"><title>Fundamental Properties of Logarithms</title><idx><h>Logarithms</h><h>Properties</h></idx><statement><p>Let <m>a</m> and <m>b</m> be positive real numbers, and <m>r</m> a real number.

<mdn>
<mrow xml:id="eq-log-prop-1">\log_2 1 = 0</mrow>
<mrow xml:id="eq-log-prop-2">\log_2 a b = \log_2a + \log_2b</mrow>
<mrow xml:id="eq-log-prop-3">\log_2 \frac{a}{b}= \log_2a - \log_2b</mrow>
<mrow xml:id="eq-log-prop-4">\log_2a^r = r \log_2a</mrow>
<mrow xml:id="eq-log-prop-5">2^{\log_2a}= a</mrow>
</mdn>
</p>
</statement></theorem>
<definition xml:id="def-logarithm-general-base"><title>Logarithms base <m>b</m></title><idx><h>Logarithm</h><h>General Base</h></idx>
<notation><usage>log_b a</usage><description>Logarithm, base <m>b</m> of <m>a</m></description></notation>
<statement><p>If \(b > 0\), \(b \neq 1\), then for \(a>0\), 
 \[\log_b a = x\Leftrightarrow b^x= a\]</p></statement>
</definition>

<theorem xml:id="theorem-logs-related">
<title>How logarithms with different bases are related</title><statement><p>Let <m>b>0</m>, \(b \neq 1\). Then for all <m>a >0</m>, \(\log_b a = \frac{\log_2a}{\log_2b}\). Therefore, if <m>b > 1</m>, base b logarithms can be computed from base 2 logarithms by dividing by the positive scaling factor \(\log_2b\). If \(b &lt; 1\), this scaling factor is negative.</p>
</statement><proof><p>By an analogue of <xref ref="eq-log-prop-5"/>, \(a=b^{\log_b a}\). Therefore, if we take the base 2 logarithm of both sides of this equality we get:
<me>\log_2 a = \log_2 \left(b^{\log_b a}\right)  \Rightarrow \log_2 a =\log_b a \cdot \log_2 b</me>
Finally, divide both sides of the last equation by \(\log_2b\).</p></proof></theorem>
 
<note><p> \(\log_210 \approx  3.32192\) and \(\log_2e = 1.55269\).</p></note>


</subsubsection>
<subsubsection><title></title>

<p>Returning to the binary search algorithm, we can derive the final expression for \(T(n)\) using the properties of logarithms, including that the
logarithm function is increasing so that inequalities are maintained when taking logarithms of numbers.</p>

<p>
<me>
\begin{split}
T(n)= r &amp; \Leftrightarrow 2^{r-1}\leq n &lt; 2^r\\
	&amp; \Leftrightarrow  \log_2 2^{r-1} \leq \log_2 n &lt; \log_2 2^r\\
	&amp; \Leftrightarrow  r-1 \leq \log_2 n &lt; r\\
	&amp; \Leftrightarrow  r-1 = \lfloor \log_2 n\rfloor \\
	&amp; \Leftrightarrow  T(n) = r= \left\lfloor \log_2 n\right\rfloor +1
\end{split}
</me>
</p>

<p>We can apply several of these properties of logarithms to get an alternate expression for \(T(n)\):
<me>
\begin{split}
\left\lfloor \log_2n\right\rfloor +1 &amp;= \left\lfloor \log_2n+1\right\rfloor \\
		&amp; = \left\lfloor \log_2n + \log_22\right\rfloor \\
		&amp;  =\text{  }\left\lfloor \log_22n \right\rfloor 
\end{split}
</me>
</p>




<p>If the time that was assigned to Step 1 of the binary search algorithm is changed, we wouldn't expect the form of the solution to be very different. If \(T(n)= a + T (\lfloor n/2 \rfloor )\) with \(T(0) = c\), then \(T(n) = c + a \left\lfloor \log_2{2n}\right\rfloor\).</p>



<p>A further generalization would be to add a coefficient to \(T(\lfloor n/2 \rfloor )\): \(T(n)= a + b T (\lfloor n/2 \rfloor )\) with \(T(0) = c\),
where \(a, b, c\in \mathbb{R}\), and \(b\neq 0\) is not quite as simple to derive. First, if we consider values of <m>n</m> that are powers of 2:



<me>
\begin{array}{c}
T(1) = a + b T(0) = a + b c\\
T(2)=a + b(a+b c) = a + a b +c b^2\\
T(4)=a+b\left(a + a b +c b^2\right) = a + a b + a b^2+ c b^3\\
\vdots \\
T\left(2^r\right) = a + a b + a b^2+\cdots  + a b^r + c b^{r+1}
\end{array}</me>



If <m>n</m> is not a power of 2, by reasoning that is identical to what we used to <xref ref="eq-bin-search-recursion"/>  and <xref ref="eq-bin-search-basis"/>,
 \[T(n) =\sum_{k=0}^r a b^k+ c b^{r+1}\]
where \(r = \left\lfloor \log_2n\right\rfloor\).</p>
<p> The first term of this expression is a geometric sum, which can be written in closed form. Let <m>x</m> be that sum:

<me>
\begin{array}{c}
  x =a + a b + a b^2+\cdots  + a b^r \\
  b x= \textrm{   }a b + a b^2+\cdots  + a b^r + a b^{r+1}
\end{array}
</me>

We've multiplied each term of <m>x</m> by <m>b</m> and aligned the identical terms in <m>x</m> and <m> bx</m>. Now if we subtract the two equations,
<me>x - b x = a - a b ^{r+1} \Rightarrow x(1-b) = a\left(1-b^{r+1}\right)</me> 
Therefore, \(x = a\frac{b^{r+1}-1}{b-1}\).</p>

<p>A closed form expression for \(T(n)\) is
<me>T(n) = a\frac{b^{r+1}-1}{b-1} +\text{  }c b^{r+1}\text{  }\text{where}\text{  }r = \left\lfloor \log_2n\right\rfloor \text{  }</me>
</p>
</subsubsection>
</subsection>


<subsection xml:id="ss-bubblesort-analysis"><title>Analysis of Bubble Sort and Merge Sort</title><idx>Bubble Sort</idx><idx>Merge Sort</idx><p>
 The efficiency of any search algorithm such as the binary search relies on fact that the search list is sorted according to a key value and that the search is based on the key value. There are several methods for sorting a list. One example is the bubble sort. You might
be familiar with this one since it is a popular <q>first sorting algorithm.</q> A time analysis of the algorithm shows that if \(B(n)\) is the worst-case
time needed to complete the bubble sort on <m>n</m> items, then \(B(n) =(n-1) + B(n-1)\) and \(B(1) = 0\). The solution of this relation is a
quadratic function \(B(n) =\frac{1}{2}\left(n^2-n\right)\). The growth rate of a quadratic function such as this one is controlled by its squared
term. Any other terms are dwarfed by it as <m>n</m> gets large. For the bubble sort, this means that if we double the size of the list that we
are to sort, <m>n</m> changes to \(2n\) and so \(n^2\) becomes \(4n^2\) . Therefore, the time needed to do a bubble sort is quadrupled. One alternative
to bubble sort is the merge sort. Here is a simple version of this algorithm for sorting \(F=\{r(1), r(2), \ldots , r(n)\}\), \(n \geq  1\). If \(n
= 1\), the list is sorted trivially. If \(n\geq  2\) then:</p>

<p><ol label="(1)">
<li><p> Divide \(F\) into \(F_1= \{r(1), \ldots , r(\lfloor n/2\rfloor )\}\) and \(F_2= \{r(\lfloor n/2\rfloor +1), \ldots ,r(n)\}\).</p></li>
<li><p>Sort \(F_1\) and \(F_2\) using a merge sort.</p></li>
<li><p>Merge the sorted lists \(F_1\) and \(F_2\) into one sorted list. If the sort is to be done in descending order of key values, you continue to choose the higher key value from the fronts of \(F_1\) and \(F_2\) and place them in the back of <m>F</m>.</p></li>
</ol></p>

<p>Note that \(F_1\) will always have \(\lfloor n/2\rfloor\) items and \(F_2\) will have \(\lceil n/2\rceil\) items; thus, if <m>n</m> is odd, \(F_2\)
gets one more item than \(F_1\). We will assume that the time required to perform Step 1 of the algorithm is insignificant compared to the other
steps; therefore, we will assign a time value of zero to this step. Step 3 requires roughly <m>n</m> comparisons and <m>n</m> movements of
items from \(F_1\) and \(F_2\) to <m>F</m>; thus, its time is proportional to <m>n</m>. For this reason, we will assume that Step 3 takes
<m>n</m> time units. Since Step 2 requires \(T(\lfloor n/2\rfloor ) + T(\lceil n/2\rceil )\) time units,
<mdn><mrow xml:id="eq-bubble-r">T(n) = n + T(\lfloor n/2\rfloor ) + T(\lceil n/2\rceil )</mrow></mdn>
with the initial condition
<mdn><mrow xml:id="eq-bubble-b">T(1) = 0</mrow></mdn>
</p>

<p>Instead of an exact solution of these equations, we will be content with an estimate for \(T(n)\).  First, consider the case of \(n=2^r\), \(r \geq 1\):



<me>
\begin{array}{c}
T\left(2^1\right)= T(2) = 2 +T(1)+T(1)= 2 = 1\cdot  2\\
T\left(2^2\right) = T(4)=4+\text{  }T(2)+T(2)=8 = 2\cdot 4\\
T\left(2^3\right) =T(8)=8 + T(4)+T(4) =24=3\cdot 8\\
\vdots \\
T\left(2^r\right)=r 2^r= 2^r \log_2 2^r
\end{array}
</me>
</p>
<p>Thus, if <m>n</m> is a power of 2, \(T(n) = n \log_2 n\). Now if, for some \(r \geq  2\), \(2^{r-1}\leq n\leq 2^r\), then \((r-1)2^{r-1}\leq
T(n) &lt; r 2^r\). This can be proved by induction on <m>r</m>. As <m>n</m> increases from \(2^{r-1}\) to \(2^r\), \(T(n)\) increases from \((r-1)2^{r-1}\)to
\(r 2^r\) and is slightly larger than \(\left\lfloor n \log_2n\right\rfloor\). The discrepancy is small enough so that \(T_e(n)=\left\lfloor n \log
_2n\right\rfloor\) can be considered a solution of <xref ref="eq-bubble-r" /> and <xref ref="eq-bubble-b"/> for the purposes of comparing the merge sort with other algorithms. <xref ref="table-sort-analysis" text="type-global" />
compares \(B(n)\) with \(T_e(n)\) for selected values of <m>n</m>.</p>

<table xml:id="table-sort-analysis">
<caption>Comparison of Times for Bubble Sort and Merge Sort</caption>
<tabular top="major" halign="center">
<row><cell>n</cell><cell><m>B(n)</m></cell><cell><m>T_e(n)</m></cell></row>
<row><cell>10</cell><cell>45</cell><cell>34</cell></row>
<row><cell>50</cell><cell>1225</cell><cell>283</cell></row>
<row><cell>100</cell><cell>4950</cell><cell>665</cell></row>
<row><cell>500</cell><cell>124750</cell><cell>4483</cell></row>
<row><cell>1000</cell><cell>499500</cell><cell>9966</cell></row>
</tabular>
</table>
</subsection>
<subsection xml:id="ss-derangements"><title>Derangements</title> <idx>Derangement</idx>
<p>A derangement is a permutation on a set that has no <q>fixed points</q>.  Here is a formal definition:</p>
<definition xml:id="def-derangement"><title>Derangement</title><statement><p>A derangement of a nonempty set \(A\) is a permutation of \(A\) (i.e., a bijection from \(A\) into \(A\)) such that \(f(a)\neq a\) for all \(a \in  A\).</p></statement></definition>




<p> If \(A = \{1, 2, . . . , n\}\), an interesting question might be <q>How many derangements are there of <m>A</m>?</q>  We know that our answer is bounded above by \(n!\). We can also expect our answer to be quite a bit smaller than \(n!\) since <m>n</m> is the image of itself for \((n-1)!\) of the permutations of <m>A</m>.</p>



<p>Let \(D(n)\) be the number of derangements of \(\{1, 2, . . . , n\}\). Our answer will come from discovering a recurrence relation on <m>D</m>. Suppose that \(n \geq  3\). If we are to construct a derangement of \(\{1, 2, \dots , n\}\), <m>f</m>, then \(f(n) = k \neq n\). Thus, the
image of <m>n</m> can be selected in \(n-1\) different ways. No matter which of the \(n -1\) choices we make, we can complete the definition of
<m>f</m> in one of two ways. First, we can decide to make \(f(k) = n\), leaving \(D(n -2)\) ways of completing the definition of \(f\), since <m>f</m> will be a derangement of \(\{1, 2, \dots ,n\} - \{n, k\}\). Second, if we decide to select \(f(k)\neq  n\), each of the
\(D(n - 1)\) derangements of \(\{1, 2,\dots ,n-1\}\) can be used to define <m>f</m>. If <m>g</m> is a derangement of \(\{1, 2, \dots , n-1\}\) such that \(g(p) = k\), then define f by
\[f(j)=\left\{
\begin{array}{cc}
 n &amp; \textrm{ if } j = p \\
 k &amp; \textrm{ if } j = n \\
 g(j) &amp; \textrm{ otherwise } \\
\end{array}
\right.\]
</p>

<p>Note that with our second construction of <m>f</m>, \(f(f(n)) = f(k) \neq  n\), while in the first construction, \(f(f(n)) = f(k) = n\). Therefore,
no derangement of \(\{1, 2, . . . , n\}\) with \(f(n) = k\) can be constructed by both methods.</p>

<p>To recap our result, we see that <m>f</m> is determined by first choosing one of \(n - 1\) images of <m>n</m> and then constructing the
remainder of <m>f</m> in one of \(D(n - 2) + D(n -1)\) ways. Therefore,

<mdn><mrow>D(n) = (n - 1) (D(n - 2) + D(n - 1))</mrow></mdn>
</p>


<p>This homogeneous second-order linear relation with variable coefficients, together with the initial conditions \(D(1) = 0\) and \(D(2) = 1\), completely
defines <m>D</m>. Instead of deriving a solution of this relation by analytical methods, we will give an empirical derivation of an approximation of \(D(n)\). Since the derangements of \(\{1,2 . . . , n\}\) are drawn from a pool of \(n!\) permutations, we will see what percentage of these permutations
are derangements by listing the values of \(n!\), \(D(n)\), and \(\frac{D(n)}{n!}\).  The results we observe will indicate that
as <m>n</m> grows, \(\frac{D(n)}{n!}\) hardly changes at all. If this quotient is computed to eight decimal places, for \(n \geq  12\), \(D(n)/n!
= 0.36787944\). The reciprocal of this number, which \(D(n)/n!\) seems to be tending toward, is, to eight places, 2.7182818. This number appears
in so many places in mathematics that it has its own name, <m>e</m>. An approximate solution of our recurrence relation on <m>D</m> is then \(D(n)\approx \frac{n!}{e}\).</p>
<sage>
<input>
def D(n):
	if n&lt;=2:
		return n-1
	else:
		return (n-1)*(D(n-2)+D(n-1))

map(lambda k:[k,D(k),(D(k)/factorial(k)).n(digits=8)],range(1,16))
</input>
<output>
[[1, 0, 0.00000000],
 [2, 1, 0.50000000],
 [3, 2, 0.33333333],
 [4, 9, 0.37500000],
 [5, 44, 0.36666667],
 [6, 265, 0.36805556],
 [7, 1854, 0.36785714],
 [8, 14833, 0.36788194],
 [9, 133496, 0.36787919],
 [10, 1334961, 0.36787946],
 [11, 14684570, 0.36787944],
 [12, 176214841, 0.36787944],
 [13, 2290792932, 0.36787944],
 [14, 32071101049, 0.36787944],
 [15, 481066515734, 0.36787944]]
</output>
</sage>
</subsection>


<exercises xml:id="exercises-8-4">
<title>Exercises for Section 8.4</title>




<exercise number="1"><statement> <p>Solve the following recurrence relations. Indicate whether your solution is an improvement over iteration.</p>
<p><ol label="(a)">
<li><p> \(n S(n) - S(n - 1) = 0\), \(S(0) = 1\).</p></li>
<li><p> \(T(k) + 3k T(k - 1) = 0\), \(T(0) = 1\).</p></li>
<li><p> \(U(k) -\frac{k-1}{k}U(k - 1) = 0\), \(k \geq  2\), \(U(1) = 1\).</p></li>
</ol></p>
</statement><answer><p><ol label="(a)"><li><p>  \(S(n)=1/n\)!  </p></li>
<li><p>\(U(k)=1/k\), an improvement. </p></li>
<li><p>  \(T(k)=(-3)^kk\)!, no improvement.</p></li>
</ol></p></answer></exercise>

<exercise number="2"><statement> <p>Prove that if \(n \geq 0\), \(\lfloor n/2\rfloor +\lceil n/2\rceil = n\). (Hint: Consider the cases of \(n\) odd and \(n\) even separately.)
</p>
</statement></exercise>



<exercise number="3"><statement><p>Solve as completely as possible:</p>
 <p><ol label="(a)">
<li><p>\(T(n) = 3 + T(\lfloor n/2\rfloor )\), \(T(0) = 0\).</p></li>
<li><p>\(T(n) = 1 + \frac{1}{2}T(\lfloor n/2\rfloor )\), \(T(0) = 2\).</p></li>
<li><p> \(V(n) = 1 + V\lfloor n/8\rfloor )\), \(V(0) = 0\). (Hint: Write <m>n</m> in octal form.)</p></li>
</ol></p>
</statement><answer><p><ol label="(a)"><li><p>  \(T(n)=3\left(\left\lfloor \log_2n\right\rfloor +1\right)\) </p></li>
<li><p>\(T(n)=2\)</p></li>
<li><p> \(V(n)=\left\lfloor \log_8n\right\rfloor +1\)</p></li>
</ol></p></answer></exercise>

<exercise number="4"><statement><p> Prove by induction that if \(T(n)= 1 + T (\lfloor n/2 \rfloor )\), \(T(0) = 0\), and \(2^{r-1}\leq n &lt; 2^r\) , \(r \geq  1\), then \(T(n) = r\).
</p>
</statement><hint><p>Prove by induction on <m>r</m>.</p></hint></exercise>

<exercise number="5"><statement><p> Use the substitution \(S(n) = T(n + 1)/T(n)\) to solve
\(T(n)T(n-2)-T(n)^2=1\) for \(n \geq  2\), with \(T(0) = 1\), \(T(1) = 6\), and \(T(n) \geq  0\).</p>
</statement><answer><p> The indicated substitution yields \(S(n)=S(n+1)\). Since \(S(0)=T(1)/T(0)=6\), \(S(n)=6\) for all \(n\). Therefore \(T(n+1)=6T(n)\Rightarrow T(n)=6^n\).</p></answer>
</exercise>

<exercise number="6"><statement><p> Use the substitution \(G(n) =T(n)^2\) to solve
\(T(n)^2-T(n-1)^2=1\) for \(n \geq  1\), with \(T(0) = 10\).</p>
</statement></exercise>

<exercise number="7"><statement><p>Solve as completely as possible:</p>
 <p><ol label="(a)">
<li><p> \(Q(n)=1+Q\left(\left\lfloor \sqrt{n}\right\rfloor \right)\), \(n \geq  2\), \(Q(1) = 0\).</p></li>
<li><p> \(R(n)=n +R(\lfloor n/2\rfloor )\), \(n \geq  1\), \(R(0) = 0\).</p></li>
</ol></p>
</statement><answer><p><ol label="(a)"><li><p> A good approximation to the solution of this recurrence relation
 is based on the following observation: \(n\) is a power of a power of two; that is,
  \(n\) is \(2^m\), where \(m=2^k\) , then \(Q(n)=1+Q\left(2^{m/2}\right)\).
   By applying this recurrence relation \(k\) times we obtain \(Q(n)=k\).
    Going back to the original form of \(n\),
     \(\log _2n=2^k\) or \(\log _2\left(\log _2n\right)=k\).
      We would expect that in general, \(Q(n)\) is \(\left\lfloor \log _2\left(\log _2n\right)\right\rfloor\).
       We do not see any elementary method for arriving at an exact solution. </p></li>
<li><p> Suppose that \(n\) is a positive integer with \(2^{k-1} \leq n &lt; 2^k\).
 Then \(n\) can be written in binary form, \(\left(a_{k-1}a_{k-2}\cdots  a_2a_1a_0\right)_{\textrm{two}}\)
  with \(a_{k-1}=1\) and \(R(n)\) is equal to the sum \(\underset{i=0}{\overset{k-1}{\Sigma }}\)
  \(\left(a_{k-1}a_{k-2}\cdots  a_i\right)_{\textrm{two}}\). If \(2^{k-1}\leq n &lt; 2^k\),
   then we can estimate this sum to be between \(2n-1\) and \(2n+1\).
    Therefore, \(R(n)\approx 2n\).</p></li></ol></p>
</answer></exercise>

<exercise number="8"><statement><p>Suppose Step 1 of the merge sort algorithm did take a significant amount of time. Assume it takes 0.1 time unit, independent of the value of
<m>n</m>.</p>
 <p><ol label="(a)">
<li><p>Write out a new recurrence relation for \(T(n)\) that takes this factor into account.</p></li>
<li><p> Solve for \(T\left(2^r\right)\), \(r \geq  0\).</p></li>
<li><p>Assuming the solution for powers of 2 is a good estimate for all \(n\), compare your result to the solution in the text. As gets large, is there
really much difference?</p></li>
</ol></p>
</statement></exercise>

</exercises>
</section>
